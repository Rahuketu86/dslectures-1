---

title: Lesson 12 - NLP classic

keywords: fastai
sidebar: home_sidebar

summary: "Introduction to Natural Language Processing (NLP)."
description: "Introduction to Natural Language Processing (NLP)."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: notebooks/lesson12_nlp-classic.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><em>TODO: Badges</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learning-objectives">Learning objectives<a class="anchor-link" href="#Learning-objectives"> </a></h2><p>In this lecture we cover the basics of NLP to build a sentiment classifier in scikit-learn. The learning goals are:</p>
<ul>
<li>Know the basics of string processing in pythin</li>
<li>Preprocessing steps in NLP</li>
<li>Count and TF-IDF encodings</li>
<li>Naïve Bayes classifier</li>
</ul>
<h2 id="References">References<a class="anchor-link" href="#References"> </a></h2><ul>
<li>Chapter 10: Representating and Mining Text <em>Data Science for Business</em> by F. Provost and P. Fawcett</li>
</ul>
<h2 id="Homework">Homework<a class="anchor-link" href="#Homework"> </a></h2><p>As homework read the references, work carefully through the notebook and solve the exercises.</p>
<h2 id="Introduction-to-NLP">Introduction to NLP<a class="anchor-link" href="#Introduction-to-NLP"> </a></h2><div style="text-align: center">
{% include image.html max-width="400" file="/dslectures/images/natural-language-processing-so-hot-right-now.jpg" %}
</div><p>Natural language processing (NLP) concerns the part of Machine Learning about the analysis of digital, human written texts. The topic of NLP is as old as machine learning itself and dates back to Alan Turing himself. Since text is a widely used medium there are plenty of applications of machine learning:</p>
<ul>
<li>Text classification</li>
<li>Question/answering systems</li>
<li>Dialogue systems</li>
<li>Named entity recognition</li>
<li>Summarization</li>
<li>Text generation</li>
</ul>
<p>Especially in the past few years there has been exciting and rapid progress in the field. One example is the release of OpenAI's GPT-2, a language model able to not only create realistic text samples but also solve tasks of many NLP benchmarks without special training. See the figure below for an example output of GPT-2.</p>
<p>If you want to try your own examples you can do so at <a href="https://talktotransformer.com/">talktotransformer.com</a> or read the original article on <a href="https://openai.com/blog/better-language-models/">OpenAI's webpage</a>.</p>
<div style="text-align: center">
{% include image.html max-width="400" file="/dslectures/images/gpt2-example.png" %}
</div><p>Natural text is different to other data sources such as numerical tables or images. One way to look at text is to consider each word to be a feature. Since most languages have of the order of 100k words in their vocabulary plus many variations this leads to an enormous feature space. At the same time most words in the vocabulary do not appear in a small text. This leads to extreme sparsity. These properties call for a different approach to NLP than the methods we encountered and used for tabular data.</p>
<h2 id="Notebook-overview">Notebook overview<a class="anchor-link" href="#Notebook-overview"> </a></h2><p>The goal of this notebook is to classify movie reviews in terms of positive or negative feedback. This task called sentiment analysis is a common NLP application. As a company you might use a sentiment classifier to analysis customer feedback or detect toxic comments on your website.</p>
<div style="text-align: center">
{% include image.html max-width="400" file="/dslectures/images/sentiment-meme.jpg" %}
</div><p>Text data can be messy and require some clean up. The specific steps for the clean-up can depend on how the data was generated or where it was found. Text from the web might have some html artifacts that need cleaning or product reviews could include meta information on the review. Python offers powerful tool to manipulate strings. If cleaning requires complex rules one can also resort to regular expressions or regex for short.</p>
<p>Once the text is cleaned we have to encode it in a way that machine learning methods can handle. Directly using text representation as input is not possible. Most machine learning methods can only handle numerical data such as vectors and matrices. So we have to encode the input texts as vectors or matrices. These text representations are called vector encodings. Furthermore, we look at n-grams to keep some of the sequential structure of text.</p>
<p>Finally, we can train a model to classify the movie review texts. However, the Random Forest models we already know well do not work well for the high-dimensional data. We introduce a new methods that is common for text data called the Naïve Bayes classifier that utilises Bayes theorem.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Imports">Imports<a class="anchor-link" href="#Imports"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="kn">from</span> <span class="nn">dslectures.core</span> <span class="kn">import</span> <span class="n">get_dataset</span>

<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="n">tqdm</span><span class="o">.</span><span class="n">pandas</span><span class="p">(</span><span class="n">desc</span><span class="o">=</span><span class="s2">&quot;progress&quot;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">nltk.stem.snowball</span> <span class="kn">import</span> <span class="n">SnowballStemmer</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span><span class="p">,</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>/Users/leandro/git/dslectures/env/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version
  from pandas import Panel
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Part-1:-Dataset">Part 1: Dataset<a class="anchor-link" href="#Part-1:-Dataset"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First, we load the IMDB dataset as a dataframe. Note that this is not the original dataset from <a href="https://ai.stanford.edu/~amaas/data/sentiment/">here</a>, but a version that I pre-processed for the ease of use.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">get_dataset</span><span class="p">(</span><span class="s1">&#39;imdb.csv&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Dataset already exists at &#39;../data/imdb.csv&#39; and is not downloaded again.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_imdb</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../data/imdb.csv&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_imdb</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>filename</th>
      <th>text</th>
      <th>sentiment</th>
      <th>train_label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4715_9</td>
      <td>For a movie that gets no respect there sure ar...</td>
      <td>pos</td>
      <td>train</td>
    </tr>
    <tr>
      <th>1</th>
      <td>12390_8</td>
      <td>Bizarre horror movie filled with famous faces ...</td>
      <td>pos</td>
      <td>train</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8329_7</td>
      <td>A solid, if unremarkable film. Matthau, as Ein...</td>
      <td>pos</td>
      <td>train</td>
    </tr>
    <tr>
      <th>3</th>
      <td>9063_8</td>
      <td>It's a strange feeling to sit alone in a theat...</td>
      <td>pos</td>
      <td>train</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3092_10</td>
      <td>You probably all already know this by now, but...</td>
      <td>pos</td>
      <td>train</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The dataset consists of a <code>filename</code>, <code>text</code>, <code>sentiment</code> and a <code>train_label</code>. The latter splits the data into a train and test set which is used as the official benchmark. We will follow that same split.</p>
<p>But first we want to make the <code>sentiment</code> column categorical:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, let's have a look at a few text examples. For that purpose we wrote a helper function to print exampeles from the dataset:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">print_n_samples</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function to plot data samples from IMDB dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;SAMPLE&#39;</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">df_sample</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">df_sample</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Sentiment:&#39;</span><span class="p">,</span> <span class="n">df_sample</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;=&#39;</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can show a few examples:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">print_n_samples</span><span class="p">(</span><span class="n">df_imdb</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>SAMPLE 1 

May contain minor spoilers.&lt;br /&gt;&lt;br /&gt;Dressed to Kill, having just seen it for the first time the other day, is a movie with some terrific sequences, some decent performances, and a nice, though obvious Hitchcock ripoff, plot twist at the end. It&#39;s just too bad certain things, quite obvious, prevent it from being a classic.&lt;br /&gt;&lt;br /&gt;Dressed to Kill deals with a mystery of a killer who has slain at least two women (could have been more but the movie never tells us), and the search for the killer by three people: a brainy kid whose mom was a victim, a lively hooker whose the only one who can identify the killer, and a psychiatrist whose patient was the slained brainy kid&#39;s mother. There are a host of well done performances including Nancy Allen as the hooker Liz, Angie Dickenson as the sexually frustrated victim Kate Miller, and Micheal Caine as Doctor Rober Elliot whose has more hidden than meets the eye; though by no standard is anyone really outstanding. On the other hand Denis Franz, later to be a great character along the same lines on the hit show NYPD Blue, is embarrassing as an over-exaggerated, ruthless, hateful detective, though he takes little screen time so it doesn&#39;t hurt the movie that much. Keith Gordon, the brain child, is decent, but is almost too smart to really be believable.&lt;br /&gt;&lt;br /&gt;What sticks with me the most in the film are the tense sequences. The scene with the sexually frustrated mother at the museum is gripping and well done, as is the later sequence leading to her untimely death. I also love the sequence at the doctor&#39;s office, that reveals the identity of the killer, which really makes the whole film come together at once. And there&#39;s a split screen scene that I thought was almost classic. Still there&#39;s another situation when Liz is on a train, running from what may be the killer, that gets taken in directions it should never had with almost embarrassing racial stereotypes. As a thriller it&#39;s tense and quite often believable. As a crime drama it often falters, especially with the incomplete, ridiculous explanation of everything that happened in the end. There&#39;s also a needless, and way too long, dream sequence at the end, in which they had the audacity to shoot someone&#39;s foot for what seemed minutes at a time. This was an up and down experience; one scene would catch me by the heart and mind with enticement, and the next would make me squirm and ask why. At more than 100 minutes this could have been under 90 and been a better film.&lt;br /&gt;&lt;br /&gt;Scary movie fans should like this. It still stands as a better film than most of today&#39;s slasher flicks. Maybe it was a better film in it&#39;s time, but having just seen it recently I can say I enjoy it, and it is worth watching for my three favorite scenes alone.

Sentiment: pos 

====================================================================================================
SAMPLE 2 

saw this in preview- great movie- wonderful characterizations- witty and intelligent dialog- actors were fantastic- Peter Falk will be up for an Oscar- Paul Reiser was charming- photography was marvelous Reiser was at the theater when we saw the film, and he gave a vivid account about the making of the film- it had been a long dream of his to write a semi-autobiographical account of relationships between sons and fathers, and more specifically between him and his father- this was achieved in a dramatic and entertaining fashion- the supporting cast was well chosen and gave the film a feeling of family- i recommend this film to anyone who is longing to see intelligent drama and wonderful performances

Sentiment: pos 

====================================================================================================
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that the reviews are medium sized texts with positive and negative labels.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Exercise-1">Exercise 1<a class="anchor-link" href="#Exercise-1"> </a></h3><p>A few exploratory and processing tasks:</p>
<ul>
<li>Create a plot with showing the distribution of positive and negative comments in the train and test dataset.</li>
<li>Study the distribution of the text lengths. You can perform string operations on by accessing the <code>str</code> object of a column: df['YOUR_TEXT_COLUMN'].str.len().</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x1238a5390&gt;</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">len</span><span class="p">()</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x123843128&gt;</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Part-2:-Preprocessing">Part 2: Preprocessing<a class="anchor-link" href="#Part-2:-Preprocessing"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this section we have a look at the basics of string processing. Being able to filter/combine/manipulate strings is a crucial skill to do natural language processing.</p>
<p>Cleaning up text for NLP tasks usually involves the following steps.</p>
<ul>
<li>Normalization</li>
<li>Tokenization</li>
<li>Remove stop-words</li>
<li>Remove non-alphabetical tokens</li>
<li>Stemming</li>
</ul>
<p>Some of the steps might not be necessary or you need to add steps depending on the text, task and method. For our task these steps are fine. We apply these steps on one text as an example and then build a function to apply it to all texts.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="String-processing-in-Python">String processing in Python<a class="anchor-link" href="#String-processing-in-Python"> </a></h3><p>Python offers powerful properties and functions to manipulate strings. The Python primer notebook offers an introduction to string processing with Python. Make sure to check it out. Once you are armed with this arsenal of string processing tools, we can preprocess the texts in the dataset to bring them to a cleaner form. Fortunately we don't need to implement everything from scratch. One of the richest Python libraries to process texts is the Natural Language Toolkit (NLTK) which offers some powerful functions we will use.</p>
<h3 id="Exercise-2">Exercise 2<a class="anchor-link" href="#Exercise-2"> </a></h3><p>Work through the string processing introduction in the Python primer notebook.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Input">Input<a class="anchor-link" href="#Input"> </a></h3><p>We see that the raw text as several features such as capitalisation, special characters, numericals and puncuations.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="n">df_imdb</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan &#34;The Skipper&#34; Hale jr. as a police Sgt.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Normalize">Normalize<a class="anchor-link" href="#Normalize"> </a></h3><p>This is the process of transforming the text to lower-case.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. imagine a movie where joe piscopo is actually funny! maureen stapleton is a scene stealer. the moroni character is an absolute scream. watch for alan &#34;the skipper&#34; hale jr. as a police sgt.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Tokenize">Tokenize<a class="anchor-link" href="#Tokenize"> </a></h3><p>Now we split the text in words/tokens.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;for&#39;, &#39;a&#39;, &#39;movie&#39;, &#39;that&#39;, &#39;gets&#39;, &#39;no&#39;, &#39;respect&#39;, &#39;there&#39;, &#39;sure&#39;, &#39;are&#39;, &#39;a&#39;, &#39;lot&#39;, &#39;of&#39;, &#39;memorable&#39;, &#39;quotes&#39;, &#39;listed&#39;, &#39;for&#39;, &#39;this&#39;, &#39;gem&#39;, &#39;.&#39;, &#39;imagine&#39;, &#39;a&#39;, &#39;movie&#39;, &#39;where&#39;, &#39;joe&#39;, &#39;piscopo&#39;, &#39;is&#39;, &#39;actually&#39;, &#39;funny&#39;, &#39;!&#39;, &#39;maureen&#39;, &#39;stapleton&#39;, &#39;is&#39;, &#39;a&#39;, &#39;scene&#39;, &#39;stealer&#39;, &#39;.&#39;, &#39;the&#39;, &#39;moroni&#39;, &#39;character&#39;, &#39;is&#39;, &#39;an&#39;, &#39;absolute&#39;, &#39;scream&#39;, &#39;.&#39;, &#39;watch&#39;, &#39;for&#39;, &#39;alan&#39;, &#39;``&#39;, &#39;the&#39;, &#39;skipper&#39;, &#34;&#39;&#39;&#34;, &#39;hale&#39;, &#39;jr.&#39;, &#39;as&#39;, &#39;a&#39;, &#39;police&#39;, &#39;sgt&#39;, &#39;.&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Stop-Words">Stop Words<a class="anchor-link" href="#Stop-Words"> </a></h3><p>Next, we remove words that are too common and don't add the the content of sentences. These words are commonly called 'stop words'. NLTK provides a list of stop words:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stop_words</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;if&#39;, &#39;did&#39;, &#39;any&#39;, &#39;an&#39;, &#39;he&#39;, &#39;wouldn&#39;, &#39;not&#39;, &#39;yours&#39;, &#39;her&#39;, &#34;needn&#39;t&#34;, &#39;doing&#39;, &#39;until&#39;, &#39;in&#39;, &#39;on&#39;, &#39;t&#39;, &#34;aren&#39;t&#34;, &#34;mustn&#39;t&#34;, &#39;our&#39;, &#39;shan&#39;, &#39;ll&#39;, &#39;o&#39;, &#39;y&#39;, &#39;once&#39;, &#34;wasn&#39;t&#34;, &#39;me&#39;, &#39;during&#39;, &#34;don&#39;t&#34;, &#39;then&#39;, &#39;against&#39;, &#39;you&#39;, &#39;him&#39;, &#39;himself&#39;, &#39;why&#39;, &#39;more&#39;, &#39;are&#39;, &#34;you&#39;re&#34;, &#39;while&#39;, &#39;they&#39;, &#39;themselves&#39;, &#34;hasn&#39;t&#34;, &#39;and&#39;, &#39;were&#39;, &#34;you&#39;d&#34;, &#34;you&#39;ll&#34;, &#39;when&#39;, &#39;most&#39;, &#39;them&#39;, &#39;have&#39;, &#39;or&#39;, &#39;the&#39;, &#39;few&#39;, &#39;too&#39;, &#39;before&#39;, &#39;wasn&#39;, &#39;their&#39;, &#34;couldn&#39;t&#34;, &#39;s&#39;, &#34;wouldn&#39;t&#34;, &#39;here&#39;, &#39;having&#39;, &#39;into&#39;, &#39;which&#39;, &#39;hers&#39;, &#39;up&#39;, &#39;was&#39;, &#39;with&#39;, &#39;other&#39;, &#39;hadn&#39;, &#39;won&#39;, &#39;out&#39;, &#39;myself&#39;, &#39;yourself&#39;, &#39;being&#39;, &#34;won&#39;t&#34;, &#39;his&#39;, &#34;doesn&#39;t&#34;, &#39;because&#39;, &#39;am&#39;, &#39;didn&#39;, &#39;who&#39;, &#39;weren&#39;, &#39;your&#39;, &#39;after&#39;, &#39;below&#39;, &#39;be&#39;, &#39;further&#39;, &#39;no&#39;, &#39;do&#39;, &#39;over&#39;, &#39;now&#39;, &#39;of&#39;, &#39;by&#39;, &#34;she&#39;s&#34;, &#39;above&#39;, &#39;shouldn&#39;, &#39;mustn&#39;, &#39;haven&#39;, &#39;re&#39;, &#39;just&#39;, &#39;should&#39;, &#34;isn&#39;t&#34;, &#39;through&#39;, &#39;that&#39;, &#39;theirs&#39;, &#39;its&#39;, &#39;has&#39;, &#39;itself&#39;, &#39;been&#39;, &#39;as&#39;, &#39;all&#39;, &#34;didn&#39;t&#34;, &#39;each&#39;, &#34;hadn&#39;t&#34;, &#39;had&#39;, &#39;ma&#39;, &#39;about&#39;, &#39;but&#39;, &#34;haven&#39;t&#34;, &#39;it&#39;, &#39;between&#39;, &#39;d&#39;, &#39;does&#39;, &#39;ain&#39;, &#39;those&#39;, &#39;mightn&#39;, &#39;so&#39;, &#39;there&#39;, &#39;down&#39;, &#39;yourselves&#39;, &#39;very&#39;, &#39;needn&#39;, &#39;can&#39;, &#39;herself&#39;, &#39;m&#39;, &#39;at&#39;, &#34;mightn&#39;t&#34;, &#39;i&#39;, &#34;you&#39;ve&#34;, &#39;where&#39;, &#39;aren&#39;, &#39;nor&#39;, &#39;don&#39;, &#34;shan&#39;t&#34;, &#39;how&#39;, &#39;some&#39;, &#39;what&#39;, &#39;she&#39;, &#39;ourselves&#39;, &#39;doesn&#39;, &#39;off&#39;, &#39;under&#39;, &#39;my&#39;, &#34;should&#39;ve&#34;, &#39;a&#39;, &#39;for&#39;, &#34;weren&#39;t&#34;, &#39;only&#39;, &#39;these&#39;, &#39;we&#39;, &#39;whom&#39;, &#34;that&#39;ll&#34;, &#39;will&#39;, &#34;shouldn&#39;t&#34;, &#39;from&#39;, &#39;to&#39;, &#39;isn&#39;, &#39;couldn&#39;, &#34;it&#39;s&#34;, &#39;own&#39;, &#39;ve&#39;, &#39;ours&#39;, &#39;same&#39;, &#39;such&#39;, &#39;than&#39;, &#39;hasn&#39;, &#39;again&#39;, &#39;is&#39;, &#39;this&#39;, &#39;both&#39;}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We keep only the words that are <strong>not</strong> in the list of stop words.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;movie&#39;, &#39;gets&#39;, &#39;respect&#39;, &#39;sure&#39;, &#39;lot&#39;, &#39;memorable&#39;, &#39;quotes&#39;, &#39;listed&#39;, &#39;gem&#39;, &#39;.&#39;, &#39;imagine&#39;, &#39;movie&#39;, &#39;joe&#39;, &#39;piscopo&#39;, &#39;actually&#39;, &#39;funny&#39;, &#39;!&#39;, &#39;maureen&#39;, &#39;stapleton&#39;, &#39;scene&#39;, &#39;stealer&#39;, &#39;.&#39;, &#39;moroni&#39;, &#39;character&#39;, &#39;absolute&#39;, &#39;scream&#39;, &#39;.&#39;, &#39;watch&#39;, &#39;alan&#39;, &#39;``&#39;, &#39;skipper&#39;, &#34;&#39;&#39;&#34;, &#39;hale&#39;, &#39;jr.&#39;, &#39;police&#39;, &#39;sgt&#39;, &#39;.&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Punctuation">Punctuation<a class="anchor-link" href="#Punctuation"> </a></h3><p>We also want to get of all tokens that are not composed of letters (e.g. punctuation and numbers). We can check if a words is only composed of alphabetic letters with the <code>isalpha()</code> and filter with it:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;movie&#39;, &#39;gets&#39;, &#39;respect&#39;, &#39;sure&#39;, &#39;lot&#39;, &#39;memorable&#39;, &#39;quotes&#39;, &#39;listed&#39;, &#39;gem&#39;, &#39;imagine&#39;, &#39;movie&#39;, &#39;joe&#39;, &#39;piscopo&#39;, &#39;actually&#39;, &#39;funny&#39;, &#39;maureen&#39;, &#39;stapleton&#39;, &#39;scene&#39;, &#39;stealer&#39;, &#39;moroni&#39;, &#39;character&#39;, &#39;absolute&#39;, &#39;scream&#39;, &#39;watch&#39;, &#39;alan&#39;, &#39;skipper&#39;, &#39;hale&#39;, &#39;police&#39;, &#39;sgt&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Stemming">Stemming<a class="anchor-link" href="#Stemming"> </a></h3><p>As a final step we want to trim the words to the stem. This helps drastically decrease the vocabulary size and maps similar/same words onto the same word. E.g. plural/singular words or different forms of verbs:</p>
<ul>
<li>pen, pens --&gt; pen</li>
<li>happy, happier --&gt; happi</li>
<li>go, goes --&gt; go</li>
</ul>
<p>There are several languages available in nltk since this is a <strong>language dependant process</strong>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">SnowballStemmer</span><span class="o">.</span><span class="n">languages</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(&#39;arabic&#39;, &#39;danish&#39;, &#39;dutch&#39;, &#39;english&#39;, &#39;finnish&#39;, &#39;french&#39;, &#39;german&#39;, &#39;hungarian&#39;, &#39;italian&#39;, &#39;norwegian&#39;, &#39;porter&#39;, &#39;portuguese&#39;, &#39;romanian&#39;, &#39;russian&#39;, &#39;spanish&#39;, &#39;swedish&#39;)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Applied to the text sample this yields:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stemmer</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;movi&#39;, &#39;get&#39;, &#39;respect&#39;, &#39;sure&#39;, &#39;lot&#39;, &#39;memor&#39;, &#39;quot&#39;, &#39;list&#39;, &#39;gem&#39;, &#39;imagin&#39;, &#39;movi&#39;, &#39;joe&#39;, &#39;piscopo&#39;, &#39;actual&#39;, &#39;funni&#39;, &#39;maureen&#39;, &#39;stapleton&#39;, &#39;scene&#39;, &#39;stealer&#39;, &#39;moroni&#39;, &#39;charact&#39;, &#39;absolut&#39;, &#39;scream&#39;, &#39;watch&#39;, &#39;alan&#39;, &#39;skipper&#39;, &#39;hale&#39;, &#39;polic&#39;, &#39;sgt&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Preprocessing-function">Preprocessing function<a class="anchor-link" href="#Preprocessing-function"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">preprocessing</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">language</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">,</span> <span class="n">stemming</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    preprocess a string and return processed tokens.</span>
<span class="sd">    args:</span>
<span class="sd">        text: text string</span>
<span class="sd">    return:</span>
<span class="sd">        tokens: list of processed and cleaned words</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="n">language</span><span class="p">))</span>
    <span class="n">stemmer</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="n">language</span><span class="p">)</span>    
    
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()]</span>
    <span class="k">if</span> <span class="n">stemming</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">tokens</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we can apply these steps to all texts. We use the <code>apply</code> function of pandas which applies a function to every entry in a DataFrame column. Since we registered <code>tqdm</code> we can use the <code>progress_apply</code> function which uses <code>apply</code> and adds a progress bar to it.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;text_processed_stemmed&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">progress_apply</span><span class="p">(</span><span class="n">preprocessing</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>progress: 100%|██████████| 50000/50000 [03:23&lt;00:00, 245.45it/s]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Part-3:-Vector-encoding">Part 3: Vector encoding<a class="anchor-link" href="#Part-3:-Vector-encoding"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Vectorizer">Vectorizer<a class="anchor-link" href="#Vectorizer"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we cleaned up and tokenized the text corpus we are now ready to encode the texts in vectors. In class we had a look at simple <strong>one-hot encodings</strong> that can be extended to count encodings and <strong>TF-IDF encodings</strong>.</p>
<p>Scikit-learn comes with functions to do both count and TF-IDF encodings on text. The interface is very similar to the classifier just the <code>predict</code> step is replace with <code>transform</code>:</p>
<div class="highlight"><pre><span></span><span class="n">count_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">your_settings</span><span class="p">)</span>
<span class="n">count_vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">your_dataset</span><span class="p">)</span>
<span class="n">vec</span> <span class="o">=</span> <span class="n">count_vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="s1">&#39;your_text&#39;</span><span class="p">)</span>
</pre></div>
<p>This creates a vectorizer that can transform texts to vectors. We can also limit the number of words take into account when building the vector. This limits the vector size and cuts off words that occur rarely. If you set <code>max_features=10000</code> only the 10000 most occurring words are used to build the vector and all rare words are excluded. This means that the encoding vector then has a dimension of 10000. For now we take all words (<code>max_features=None</code>). Since we used our own tokenizer and preprocessing step we overwrite the standard steps in the vectorizer library with the <code>vec_default_settings</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vec_default_settings</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;analyzer&#39;</span><span class="p">:</span><span class="s1">&#39;word&#39;</span><span class="p">,</span> <span class="s1">&#39;tokenizer&#39;</span><span class="p">:</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;preprocessor&#39;</span><span class="p">:</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;token_pattern&#39;</span><span class="p">:</span><span class="kc">None</span><span class="p">,}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tfidf_vec</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">vec_default_settings</span><span class="p">)</span>
<span class="n">count_vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">vec_default_settings</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's test both vectorizers on a small, dummy dataset with <strong>4 documents</strong>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="s1">&#39;this&#39;</span><span class="p">,</span><span class="s1">&#39;is&#39;</span><span class="p">,</span><span class="s1">&#39;the&#39;</span><span class="p">,</span><span class="s1">&#39;first&#39;</span><span class="p">,</span><span class="s1">&#39;document&#39;</span><span class="p">,</span><span class="s1">&#39;in&#39;</span><span class="p">,</span><span class="s1">&#39;the&#39;</span><span class="p">,</span><span class="s1">&#39;corpus&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;this&#39;</span><span class="p">,</span><span class="s1">&#39;document&#39;</span><span class="p">,</span><span class="s1">&#39;is&#39;</span><span class="p">,</span><span class="s1">&#39;the&#39;</span><span class="p">,</span><span class="s1">&#39;second&#39;</span><span class="p">,</span><span class="s1">&#39;document&#39;</span><span class="p">,</span><span class="s1">&#39;in&#39;</span><span class="p">,</span><span class="s1">&#39;the&#39;</span><span class="p">,</span><span class="s1">&#39;corpus&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;and&#39;</span><span class="p">,</span><span class="s1">&#39;this&#39;</span><span class="p">,</span><span class="s1">&#39;is&#39;</span><span class="p">,</span><span class="s1">&#39;the&#39;</span><span class="p">,</span><span class="s1">&#39;third&#39;</span><span class="p">,</span><span class="s1">&#39;one&#39;</span><span class="p">,</span><span class="s1">&#39;in&#39;</span><span class="p">,</span><span class="s1">&#39;this&#39;</span><span class="p">,</span><span class="s1">&#39;corpus&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;is&#39;</span><span class="p">,</span><span class="s1">&#39;this&#39;</span><span class="p">,</span><span class="s1">&#39;the&#39;</span><span class="p">,</span><span class="s1">&#39;first&#39;</span><span class="p">,</span><span class="s1">&#39;document&#39;</span><span class="p">,</span><span class="s1">&#39;in&#39;</span><span class="p">,</span><span class="s1">&#39;this&#39;</span><span class="p">,</span><span class="s1">&#39;corpus&#39;</span><span class="p">],</span>
<span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we fit a count vectorizer to the data.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">count_vec</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>CountVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;,
                dtype=&lt;class &#39;numpy.int64&#39;&gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;,
                lowercase=True, max_df=1.0, max_features=None, min_df=1,
                ngram_range=(1, 1),
                preprocessor=&lt;function &lt;lambda&gt; at 0x120bf9268&gt;,
                stop_words=None, strip_accents=None, token_pattern=None,
                tokenizer=&lt;function &lt;lambda&gt; at 0x120bf92f0&gt;, vocabulary=None)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Once the a vectorizer is fitted, we can investigate the vocabulary. It is a dictionary that points each word to the index in the vector it corresponds to. For example the word <code>'this'</code> corresponds to the 10+1-nth (+1 because we start counting at zero) entry in the vector and the word <code>'and'</code> corresponds to the the first entry.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">count_vec</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>11</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">count_vec</span><span class="o">.</span><span class="n">vocabulary_</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;this&#39;: 10,
 &#39;is&#39;: 5,
 &#39;the&#39;: 8,
 &#39;first&#39;: 3,
 &#39;document&#39;: 2,
 &#39;in&#39;: 4,
 &#39;corpus&#39;: 1,
 &#39;second&#39;: 7,
 &#39;and&#39;: 0,
 &#39;third&#39;: 9,
 &#39;one&#39;: 6}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can transform the corpus and get a list of vectors in the form of a matrix (each row corresponds to a document vector):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">count_vec</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[[0 1 1 1 1 1 0 0 2 0 1]
 [0 1 2 0 1 1 0 1 2 0 1]
 [1 1 0 0 1 1 1 0 1 1 2]
 [0 1 1 1 1 1 0 0 1 0 2]]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we now do the same thing with the TF-IDF vectorizer we see that the output looks different:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tfidf_vec</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tfidf_vec</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[[0.         0.29137467 0.35639305 0.44021632 0.29137467 0.29137467
  0.         0.         0.58274934 0.         0.29137467]
 [0.         0.23798402 0.58217725 0.         0.23798402 0.23798402
  0.         0.45604658 0.47596805 0.         0.23798402]
 [0.43943636 0.22931612 0.         0.         0.22931612 0.22931612
  0.43943636 0.         0.22931612 0.43943636 0.45863224]
 [0.         0.29137467 0.35639305 0.44021632 0.29137467 0.29137467
  0.         0.         0.29137467 0.         0.58274934]]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The shape of the matrix is the same.</li>
<li>Instead of integers (corresponding to counts) we have continous values.</li>
<li>Elements that occur in multilple documents have lower scores than those appearing in fewer.</li>
</ul>
<p>This should just illustrate how count and TF-IDF vectorizer work. Now let's apply this to our dataset and create encodigs with <code>100000</code> words:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="n-grams">n-grams<a class="anchor-link" href="#n-grams"> </a></h3><p>When we use a count or TF-IDF vectorizer we through away all sequential information in the texts. From the vector encodings above we could not reconstruct the original sentences. For this reason these encodings are called Bag-of-Words encodings (all words go in a bag and are shuffeled). However, sequential information can be important for the meaning of a sentence. As an example imagine the sentence:</p>
<div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;The movie was good and not bad.&#39;</span>
</pre></div>
<p>It is important to know if the word <code>'not'</code> is in front of <code>'good'</code> or <code>'bad'</code> for determining the sentiment of the sentence. We can preserve some of that information by using n-grams. Instead of just encoding single words we can also encode tuple, triplets etc. called n-grams. The n encodes how many words we bundle together.</p>
<p>The vectorizers can do this for us if we provide them a range of n's we want to include. In the following example we encode the text in 1- and 2-grams.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">count_vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="o">**</span><span class="n">vec_default_settings</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">count_vec</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>CountVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;,
                dtype=&lt;class &#39;numpy.int64&#39;&gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;,
                lowercase=True, max_df=1.0, max_features=None, min_df=1,
                ngram_range=(1, 2),
                preprocessor=&lt;function &lt;lambda&gt; at 0x120bf9268&gt;,
                stop_words=None, strip_accents=None, token_pattern=None,
                tokenizer=&lt;function &lt;lambda&gt; at 0x120bf92f0&gt;, vocabulary=None)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that this drastically increases the vocabulary.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">count_vec</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>30</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now the vocabulary also contains word tuples next to the words:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">count_vec</span><span class="o">.</span><span class="n">vocabulary_</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;this&#39;: 25,
 &#39;is&#39;: 11,
 &#39;the&#39;: 18,
 &#39;first&#39;: 6,
 &#39;document&#39;: 3,
 &#39;in&#39;: 8,
 &#39;corpus&#39;: 2,
 &#39;this is&#39;: 28,
 &#39;is the&#39;: 12,
 &#39;the first&#39;: 20,
 &#39;first document&#39;: 7,
 &#39;document in&#39;: 4,
 &#39;in the&#39;: 9,
 &#39;the corpus&#39;: 19,
 &#39;second&#39;: 16,
 &#39;this document&#39;: 27,
 &#39;document is&#39;: 5,
 &#39;the second&#39;: 21,
 &#39;second document&#39;: 17,
 &#39;and&#39;: 0,
 &#39;third&#39;: 23,
 &#39;one&#39;: 14,
 &#39;and this&#39;: 1,
 &#39;the third&#39;: 22,
 &#39;third one&#39;: 24,
 &#39;one in&#39;: 15,
 &#39;in this&#39;: 10,
 &#39;this corpus&#39;: 26,
 &#39;is this&#39;: 13,
 &#39;this the&#39;: 29}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The encodings look very similar but are larger due to the larger vocabulary:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">count_vec</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[[0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 2 1 1 0 0 0 0 1 0 0 1 0]
 [0 0 1 2 1 1 0 0 1 1 0 1 1 0 0 0 1 1 2 1 0 1 0 0 0 1 0 1 0 0]
 [1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 2 1 0 1 0]
 [0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 2 1 0 0 1]]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Vectorize-dataset">Vectorize dataset<a class="anchor-link" href="#Vectorize-dataset"> </a></h3><p>Now we want to encode the real text. We put an upper limit on the vocabulary size. Since there a lot of unique words in our corpus there are a lot of combinations of words.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">max_features</span><span class="o">=</span><span class="mi">100000</span>
<span class="n">ngrams</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>

<span class="n">count_vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="n">ngrams</span><span class="p">,</span> <span class="o">**</span><span class="n">vec_default_settings</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the example above we used the <code>fit</code> and <code>transform</code> function. We can avoid these two steps with the combined function <code>fit_transform</code>. First we need to split the dataset:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">text_train</span> <span class="o">=</span> <span class="n">df_imdb</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;train_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;text_processed_stemmed&#39;</span><span class="p">]</span>
<span class="n">text_test</span> <span class="o">=</span> <span class="n">df_imdb</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;train_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="s1">&#39;text_processed_stemmed&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">count_vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">count_vec</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_test</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This yields a vocabulary with <code>100000</code> entries:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">count_vec</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>100000</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Looking at the shape of the returned matrix we see that it still has as many rows as the input but now has <code>100000</code> entries per row (the feature vector).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(25000, 100000)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Part-4:-Na&#239;ve-Bayes-classifier">Part 4: Na&#239;ve Bayes classifier<a class="anchor-link" href="#Part-4:-Na&#239;ve-Bayes-classifier"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we featurised the text we can train a model on it. In this section we will use a Naïve Bayes classifier to determine whether a review is positive or negative. Firs we split the labels of the training and test set.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_train</span> <span class="o">=</span> <span class="n">df_imdb</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;train_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;sentiment&#39;</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">df_imdb</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;train_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="s1">&#39;sentiment&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can initialise a Naïve Bayes classifier the same way we initialised the Random Forest models. Also the fit/predict interface is the same.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">nb_clf</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">nb_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">nb_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can calculate the prediction accuracy on the test set:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.85192</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Custom-prediction">Custom prediction<a class="anchor-link" href="#Custom-prediction"> </a></h3><p>Now that we have a model we want to use it to make some custom predictions. We can build an easy pipeline that preprocesses, vectorises and predicts:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;This movie sucked!!&#39;</span><span class="p">,</span>
         <span class="s1">&#39;This movie is awesome :)&#39;</span><span class="p">,</span>
         <span class="s1">&#39;I did not like that movie at all.&#39;</span><span class="p">,</span>
         <span class="s1">&#39;The movie was boring.&#39;</span><span class="p">]</span>

<span class="n">enc</span> <span class="o">=</span> <span class="n">count_vec</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">preprocessing</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">nb_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">enc</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;neg&#39; &#39;pos&#39; &#39;neg&#39; &#39;neg&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Exercise-3">Exercise 3<a class="anchor-link" href="#Exercise-3"> </a></h3><p>Retrain the model with a TF-IDF encoding instead of the count encoding. Experiment with the n-gram setting and run the experiment with the following settings: <code>(1,1)</code>, <code>(1,2)</code>, <code>(1,3)</code> and <code>(1,4)</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">max_features</span><span class="o">=</span><span class="mi">100000</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>

    <span class="n">ngrams</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="p">)</span>

    <span class="n">tfidf_vec</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="n">ngrams</span><span class="p">,</span> <span class="o">**</span><span class="n">vec_default_settings</span><span class="p">)</span>

    <span class="n">X_train</span> <span class="o">=</span> <span class="n">tfidf_vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">tfidf_vec</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_test</span><span class="p">)</span>

    <span class="n">nb_clf</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>

    <span class="n">nb_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">nb_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;ngrams: </span><span class="si">{ngrams}</span><span class="s1">, accuracy: {accuracy_score(y_test, y_pred)}&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>ngrams: (1, 1), accuracy: 0.82152
ngrams: (1, 2), accuracy: 0.85636
ngrams: (1, 3), accuracy: 0.85952
ngrams: (1, 4), accuracy: 0.85988
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

