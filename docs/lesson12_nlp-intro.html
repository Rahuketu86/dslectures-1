---

title: Lesson 12 - Introduction to NLP

keywords: fastai
sidebar: home_sidebar

summary: "Introduction to Natural Language Processing (NLP)."
description: "Introduction to Natural Language Processing (NLP)."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: notebooks/lesson12_nlp-intro.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://mybinder.org/v2/gh/lewtun/dslectures/master?urlpath=lab/tree/notebooks%2Flesson12_nlp-intro.ipynb"><img src="https://mybinder.org/badge_logo.svg" alt="Binder"></a> <a href="https://drive.google.com/open?id=11m5iXGNJEUlvjSMLdQAztJyVZ2LLj4oz"><img src="https://img.shields.io/static/v1?label=slides&amp;message=lesson12_nlp-intro.pdf&amp;color=blue&amp;logo=Google-drive" alt="slides"></a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Learning-objectives">Learning objectives<a class="anchor-link" href="#Learning-objectives"> </a></h2><p>In this lecture we cover the basics of NLP to build a sentiment classifier in scikit-learn. The learning goals are:</p>
<ul>
<li>Know the basics of string processing in python</li>
<li>Preprocessing steps in NLP</li>
<li>Count and TF-IDF encodings</li>
<li>Naïve Bayes classifier</li>
</ul>
<h2 id="References">References<a class="anchor-link" href="#References"> </a></h2><ul>
<li>Chapter 10: Representing and Mining Text in <em>Data Science for Business</em> by F. Provost and P. Fawcett</li>
</ul>
<h2 id="Homework">Homework<a class="anchor-link" href="#Homework"> </a></h2><p>As homework read the references, work carefully through the notebook and solve the exercises.</p>
<h2 id="Introduction-to-NLP">Introduction to NLP<a class="anchor-link" href="#Introduction-to-NLP"> </a></h2><div style="text-align: center">
{% include image.html max-width="400" file="/dslectures/images/natural-language-processing-so-hot-right-now.jpg" %}
</div><p>Natural language processing (NLP) concerns the part of Machine Learning about the analysis of digital, human written texts. The topic of NLP is as old as machine learning itself and dates back to Alan Turing himself. Since text is a widely used medium there are plenty of applications of machine learning:</p>
<ul>
<li>Text classification</li>
<li>Question/answering systems</li>
<li>Dialogue systems</li>
<li>Named entity recognition</li>
<li>Summarization</li>
<li>Text generation</li>
</ul>
<p>Especially in the past few years there has been exciting and rapid progress in the field. One example is the release of OpenAI's GPT-2, a language model able to not only create realistic text samples but also solve tasks of many NLP benchmarks without special training. See the figure below for an example output of GPT-2.</p>
<p>If you want to try your own examples you can do so at <a href="https://talktotransformer.com/">talktotransformer.com</a> or read the original article on <a href="https://openai.com/blog/better-language-models/">OpenAI's webpage</a>.</p>
<div style="text-align: center">
{% include image.html max-width="400" file="/dslectures/images/gpt2-example.png" %}
</div><p>Natural text is different to other data sources such as numerical tables or images. One way to look at text is to consider each word to be a feature. Since most languages have of the order of 100k words in their vocabulary plus many variations this leads to an enormous feature space. At the same time most words in the vocabulary do not appear in a small text. This leads to extreme sparsity. These properties call for a different approach to NLP than the methods we encountered and used for tabular data.</p>
<h2 id="Notebook-overview">Notebook overview<a class="anchor-link" href="#Notebook-overview"> </a></h2><p>The goal of this notebook is to classify movie reviews in terms of positive or negative feedback. This task is called sentiment analysis and is a common NLP application. As a company you might use a sentiment classifier to analyse customer feedback or detect toxic comments on your website.</p>
<div style="text-align: center">
{% include image.html max-width="400" file="/dslectures/images/sentiment-meme.jpg" %}
</div><p>Text data can be messy and require some clean up. The specific steps for the clean-up can depend on how the data was generated or where it was found. Text from the web might have some html artifacts that need cleaning or product reviews could include meta information on the review. Python offers powerful tools to manipulate strings. If cleaning requires complex rules one can also resort to regular expressions or regex for short.</p>
<p>Once the text is cleaned we have to encode it in a way that machine learning methods can handle. Directly using text representation as input is not possible. Most machine learning methods can only handle numerical data such as vectors and matrices. So we have to encode the input texts as vectors or matrices. These text representations are called vector encodings. Furthermore, we look at n-grams to keep some of the sequential structure of text.</p>
<p>Finally, we can train a model to classify the movie review texts. However, the Random Forest models we already know well do not work well for the high-dimensional data. We introduce a new methods that is common for text data called the Naïve Bayes classifier that utilises Bayes theorem.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Imports">Imports<a class="anchor-link" href="#Imports"> </a></h2>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pickle</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">dslectures.core</span> <span class="kn">import</span> <span class="n">get_dataset</span>

<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="n">tqdm</span><span class="o">.</span><span class="n">pandas</span><span class="p">(</span><span class="n">desc</span><span class="o">=</span><span class="s2">&quot;progress&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">nltk</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;stopwords&#39;</span><span class="p">)</span>
<span class="n">nltk</span><span class="o">.</span><span class="n">download</span><span class="p">(</span><span class="s1">&#39;punkt&#39;</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">nltk</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="kn">from</span> <span class="nn">nltk.stem.snowball</span> <span class="kn">import</span> <span class="n">SnowballStemmer</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">cosine_similarity</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span><span class="p">,</span> <span class="n">TfidfVectorizer</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[nltk_data] Downloading package stopwords to
[nltk_data]     /Users/leandro/nltk_data...
[nltk_data]   Unzipping corpora/stopwords.zip.
[nltk_data] Downloading package punkt to /Users/leandro/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Part-1:-Dataset">Part 1: Dataset<a class="anchor-link" href="#Part-1:-Dataset"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First, we load the IMDB dataset as a dataframe. Note that this is not the original dataset from <a href="https://ai.stanford.edu/~amaas/data/sentiment/">here</a>, but a version that I pre-processed for the ease of use.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">get_dataset</span><span class="p">(</span><span class="s1">&#39;imdb.csv&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Download of imdb.csv dataset complete.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_imdb</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../data/imdb.csv&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_imdb</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>filename</th>
      <th>text</th>
      <th>sentiment</th>
      <th>train_label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4715_9</td>
      <td>For a movie that gets no respect there sure ar...</td>
      <td>pos</td>
      <td>train</td>
    </tr>
    <tr>
      <th>1</th>
      <td>12390_8</td>
      <td>Bizarre horror movie filled with famous faces ...</td>
      <td>pos</td>
      <td>train</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8329_7</td>
      <td>A solid, if unremarkable film. Matthau, as Ein...</td>
      <td>pos</td>
      <td>train</td>
    </tr>
    <tr>
      <th>3</th>
      <td>9063_8</td>
      <td>It's a strange feeling to sit alone in a theat...</td>
      <td>pos</td>
      <td>train</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3092_10</td>
      <td>You probably all already know this by now, but...</td>
      <td>pos</td>
      <td>train</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The dataset consists of a <code>filename</code>, <code>text</code>, <code>sentiment</code> and a <code>train_label</code>. The latter splits the data into a train and test set which is used as the official benchmark. We will follow that same split.</p>
<p>But first we want to make the <code>sentiment</code> column categorical:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now, let's have a look at a few text examples. For that purpose we wrote a helper function to print examples from the dataset:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">print_n_samples</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function to print data samples from IMDB dataset.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;SAMPLE&#39;</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">df_sample</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">df_sample</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Sentiment:&#39;</span><span class="p">,</span> <span class="n">df_sample</span><span class="p">[</span><span class="s1">&#39;sentiment&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="p">[</span><span class="s1">&#39;=&#39;</span><span class="p">]))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can show a few examples:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">print_n_samples</span><span class="p">(</span><span class="n">df_imdb</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>SAMPLE 1 

I have just seen Today You Die. It is bad, almost very bad.&lt;br /&gt;&lt;br /&gt;1) The direction and editing are awful, just awful. Almost made me turn off the movie, Fauntleroy (the director) has no idea what he is doing, he seems to be filming things at random and some scenes don&#39;t make sense at all. Also, I hate it when the same scene is used again in the same movie, in this movie some scenes were used 3 or 4 times. Pretty bad.&lt;br /&gt;&lt;br /&gt;2) The dialogue is sometimes good, sometimes awful. I like the fact that they wanted to make Seagal&#39;s character and Treach&#39;s character seem like they were in a similar relationship to the characters in Lethal Weapon, but it did not work simply because some of the dialogue DID NOT MAKE SENSE, and I speak English very well, it&#39;s not that I did not understand the words, it was the fact that the jokes and dialogue lines had no meaning whatsoever.&lt;br /&gt;&lt;br /&gt;3) The script is pretty bad. Why do they always try to complicate DTV action movies? Seagal&#39;s wife in the movie has psychic abilities, why? Is it useful to the movie? NO. Seagal eliminates a whole bunch of people who work for the guy who betrayed him and he knows these people without having ever met them in the movie. STUPID. The story sometimes goes off track and the jumps back without any reason. The story is messy and pointless sometimes. They should have kept it simple and it would have worked.&lt;br /&gt;&lt;br /&gt;4) In some of the action scenes it is not Seagal, it is his stunt double. You can tell because they only film him from behind and never show his face. He also beats the guys with movie martial arts, not real ones like the aikido Steven knows. The stunt double uses cheesy kicks and punches.&lt;br /&gt;&lt;br /&gt;5) Steven is good in the movie. 90-95% of the lines are said with his real voice. The rest is dubbing but it is not that bad. This was good. Also Steven seems to be enjoying himself in the movie and is more into the action that he was in Submerged. He likes Treach as a partner; at least he does not seem to dislike him. Also, he seems to have been in better shape than in some of his recent movies. I hate the fact that he wears clothes to hide his body, but in the same clothes that he wears on the DVD cover he looks more than OK and he should have wore those clothes for most of the movie not the stupid long leather coat.&lt;br /&gt;&lt;br /&gt;I really think that Seagal was willing to make a good movie. The fact that he came late and took off early from the set ON TWO MOVIES directed by Fauntleroy does not look like a coincidence to me. I think he realized that the crew were amateurs or only in it for a quick buck and he did not give a damn anymore.&lt;br /&gt;&lt;br /&gt;In the hands of a better company and crew this might have been a damn good action movie for Seagal. Something like Out for Justice or Above the Law. I honestly believe that. But the people who made the movie are not very good at their jobs or they did not have enough money to do the job properly. Too bad since I liked Steven in the movie and Treach was cool (Ice Cool ) too, but the rest was bad. Hey, at least this gives me hope for Black Dawn and Shadows of the past. I think that Mercenary might be just as badly handled. But hey, Steven seemed to be back into the same mood he was in while making his better movies and at least THAT is reason enough to watch the movie.&lt;br /&gt;&lt;br /&gt;I liked it, but it could have been SO much better. 4/10

Sentiment: neg 

====================================================================================================
SAMPLE 2 

Boring, long, pretentious, repetitive, self-involved  this move felt like a bad date. Worse, the tedious art-school direction -- with a heavy-handed use of the whirling shot that gets so overdone it almost made me throw up - is constantly screaming to be noticed. Add the thinnest of plots and virtually no dialogue, and the film begins to feel like a four hour epic about 30 minutes in. It gets worse: instead of dialogue there are poorly written voice-overs AND quotes and songs that comment all too obviously on the characters. Really loud opera music too. Blame it all on the director.&lt;br /&gt;&lt;br /&gt;The actors are all quite good. The lead actor Miguel Angel Hoppe is particularly suited for film stardom. He and the other actors have some tender erotic moments. Even these start to get boring after 5 minutes however, and one wonders if the director is auditioning for a Bel Ami porn job. The stunning college campus architecture as a location in Mexico City is inspiring. How come universities in the US are so bland (SFSU, UC, etc.)? But wait for the DVD on this film. You&#39;ll want to use the fast scan button  a lot.

Sentiment: neg 

====================================================================================================
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that the reviews are medium sized texts with positive and negative labels.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Exercise-1">Exercise 1<a class="anchor-link" href="#Exercise-1"> </a></h3><p>A few exploratory and processing tasks:</p>
<ul>
<li>Create a plot with showing the distribution of positive and negative comments in the train and test dataset.</li>
<li>Study the distribution of the text lengths. You can perform string operations on a <code>pandas.DataFrame</code> by accessing the <code>str</code> object of a column: <code>df['YOUR_TEXT_COLUMN'].str.len()</code>.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Part-2:-Preprocessing">Part 2: Preprocessing<a class="anchor-link" href="#Part-2:-Preprocessing"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this section we have a look at the basics of string processing. Being able to filter/combine/manipulate strings is a crucial skill to do natural language processing.</p>
<p>Cleaning up text for NLP tasks usually involves the following steps.</p>
<ul>
<li>Normalization</li>
<li>Tokenization</li>
<li>Remove stop-words</li>
<li>Remove non-alphabetical tokens</li>
<li>Stemming</li>
</ul>
<p>Some of the steps might not be necessary or you need to add steps depending on the text, task and method. For our task these steps are fine. We apply these steps on one text as an example and then build a function to apply it to all texts.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="String-processing-in-Python">String processing in Python<a class="anchor-link" href="#String-processing-in-Python"> </a></h3><p>Python offers powerful properties and functions to manipulate strings. The Python primer notebook offers an introduction to string processing with Python. Make sure to check it out. Once you are armed with this arsenal of string processing tools, we can preprocess the texts in the dataset to bring them to a cleaner form. Fortunately we don't need to implement everything from scratch. One of the richest Python libraries to process texts is the Natural Language Toolkit (NLTK) which offers some powerful functions we will use.</p>
<h3 id="Exercise-2">Exercise 2<a class="anchor-link" href="#Exercise-2"> </a></h3><p>Work through the string processing introduction in the Python primer notebook.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Input">Input<a class="anchor-link" href="#Input"> </a></h3><p>We see that the raw text as several features such as capitalisation, special characters, numericals and puncuations.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="n">df_imdb</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan &#34;The Skipper&#34; Hale jr. as a police Sgt.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Normalize">Normalize<a class="anchor-link" href="#Normalize"> </a></h3><p>This is the process of transforming the text to lower-case.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>for a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. imagine a movie where joe piscopo is actually funny! maureen stapleton is a scene stealer. the moroni character is an absolute scream. watch for alan &#34;the skipper&#34; hale jr. as a police sgt.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Tokenize">Tokenize<a class="anchor-link" href="#Tokenize"> </a></h3><p>Now we split the text in words/tokens.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;for&#39;, &#39;a&#39;, &#39;movie&#39;, &#39;that&#39;, &#39;gets&#39;, &#39;no&#39;, &#39;respect&#39;, &#39;there&#39;, &#39;sure&#39;, &#39;are&#39;, &#39;a&#39;, &#39;lot&#39;, &#39;of&#39;, &#39;memorable&#39;, &#39;quotes&#39;, &#39;listed&#39;, &#39;for&#39;, &#39;this&#39;, &#39;gem&#39;, &#39;.&#39;, &#39;imagine&#39;, &#39;a&#39;, &#39;movie&#39;, &#39;where&#39;, &#39;joe&#39;, &#39;piscopo&#39;, &#39;is&#39;, &#39;actually&#39;, &#39;funny&#39;, &#39;!&#39;, &#39;maureen&#39;, &#39;stapleton&#39;, &#39;is&#39;, &#39;a&#39;, &#39;scene&#39;, &#39;stealer&#39;, &#39;.&#39;, &#39;the&#39;, &#39;moroni&#39;, &#39;character&#39;, &#39;is&#39;, &#39;an&#39;, &#39;absolute&#39;, &#39;scream&#39;, &#39;.&#39;, &#39;watch&#39;, &#39;for&#39;, &#39;alan&#39;, &#39;``&#39;, &#39;the&#39;, &#39;skipper&#39;, &#34;&#39;&#39;&#34;, &#39;hale&#39;, &#39;jr.&#39;, &#39;as&#39;, &#39;a&#39;, &#39;police&#39;, &#39;sgt&#39;, &#39;.&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Stop-Words">Stop Words<a class="anchor-link" href="#Stop-Words"> </a></h3><p>Next, we remove words that are too common and don't add the the content of sentences. These words are commonly called 'stop words'. NLTK provides a list of stop words:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="s1">&#39;english&#39;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">stop_words</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>{&#39;shan&#39;, &#39;some&#39;, &#39;couldn&#39;, &#39;their&#39;, &#39;them&#39;, &#39;just&#39;, &#39;me&#39;, &#39;haven&#39;, &#39;didn&#39;, &#39;her&#39;, &#39;have&#39;, &#39;ours&#39;, &#39;this&#39;, &#39;again&#39;, &#34;should&#39;ve&#34;, &#39;my&#39;, &#39;it&#39;, &#39;below&#39;, &#39;why&#39;, &#39;few&#39;, &#39;won&#39;, &#39;t&#39;, &#39;that&#39;, &#39;you&#39;, &#39;i&#39;, &#39;then&#39;, &#39;wouldn&#39;, &#39;while&#39;, &#39;ve&#39;, &#39;own&#39;, &#39;in&#39;, &#34;won&#39;t&#34;, &#39;themselves&#39;, &#34;hasn&#39;t&#34;, &#39;both&#39;, &#34;you&#39;re&#34;, &#39;at&#39;, &#39;he&#39;, &#39;they&#39;, &#39;a&#39;, &#39;nor&#39;, &#39;him&#39;, &#39;aren&#39;, &#39;mightn&#39;, &#39;now&#39;, &#39;ma&#39;, &#39;wasn&#39;, &#34;shouldn&#39;t&#34;, &#39;theirs&#39;, &#39;doesn&#39;, &#34;couldn&#39;t&#34;, &#39;than&#39;, &#39;as&#39;, &#34;weren&#39;t&#34;, &#34;you&#39;ll&#34;, &#39;during&#39;, &#39;not&#39;, &#34;hadn&#39;t&#34;, &#39;these&#39;, &#39;isn&#39;, &#39;y&#39;, &#39;which&#39;, &#39;such&#39;, &#39;yourself&#39;, &#39;where&#39;, &#39;any&#39;, &#39;your&#39;, &#39;been&#39;, &#39;what&#39;, &#39;herself&#39;, &#39;we&#39;, &#34;wasn&#39;t&#34;, &#39;over&#39;, &#39;be&#39;, &#39;an&#39;, &#39;yourselves&#39;, &#39;each&#39;, &#39;more&#39;, &#39;most&#39;, &#39;did&#39;, &#39;same&#39;, &#39;for&#39;, &#39;down&#39;, &#39;is&#39;, &#34;wouldn&#39;t&#34;, &#39;itself&#39;, &#39;by&#39;, &#39;before&#39;, &#39;so&#39;, &#39;between&#39;, &#39;from&#39;, &#39;re&#39;, &#39;m&#39;, &#34;you&#39;d&#34;, &#39;on&#39;, &#39;above&#39;, &#39;do&#39;, &#39;had&#39;, &#39;off&#39;, &#39;d&#39;, &#34;aren&#39;t&#34;, &#39;needn&#39;, &#34;you&#39;ve&#34;, &#39;yours&#39;, &#39;doing&#39;, &#39;the&#39;, &#39;has&#39;, &#39;there&#39;, &#39;too&#39;, &#39;further&#39;, &#34;she&#39;s&#34;, &#39;because&#39;, &#39;once&#39;, &#39;shouldn&#39;, &#39;being&#39;, &#39;of&#39;, &#39;will&#39;, &#39;s&#39;, &#34;haven&#39;t&#34;, &#39;o&#39;, &#39;his&#39;, &#34;isn&#39;t&#34;, &#39;whom&#39;, &#39;should&#39;, &#39;all&#39;, &#34;that&#39;ll&#34;, &#39;those&#39;, &#39;very&#39;, &#39;after&#39;, &#39;don&#39;, &#39;hers&#39;, &#39;mustn&#39;, &#39;and&#39;, &#39;were&#39;, &#34;doesn&#39;t&#34;, &#39;or&#39;, &#34;needn&#39;t&#34;, &#39;its&#39;, &#39;weren&#39;, &#39;myself&#39;, &#39;was&#39;, &#39;to&#39;, &#39;no&#39;, &#34;don&#39;t&#34;, &#39;does&#39;, &#39;ll&#39;, &#39;hadn&#39;, &#34;it&#39;s&#34;, &#39;if&#39;, &#39;when&#39;, &#39;until&#39;, &#39;through&#39;, &#34;shan&#39;t&#34;, &#39;with&#39;, &#39;are&#39;, &#39;himself&#39;, &#39;about&#39;, &#39;here&#39;, &#39;how&#39;, &#39;only&#39;, &#39;can&#39;, &#34;mightn&#39;t&#34;, &#34;mustn&#39;t&#34;, &#39;against&#39;, &#39;but&#39;, &#39;who&#39;, &#39;she&#39;, &#39;up&#39;, &#39;under&#39;, &#39;other&#39;, &#39;into&#39;, &#39;am&#39;, &#34;didn&#39;t&#34;, &#39;hasn&#39;, &#39;ain&#39;, &#39;having&#39;, &#39;ourselves&#39;, &#39;out&#39;, &#39;our&#39;}
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We keep only the words that are <strong>not</strong> in the list of stop words.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;movie&#39;, &#39;gets&#39;, &#39;respect&#39;, &#39;sure&#39;, &#39;lot&#39;, &#39;memorable&#39;, &#39;quotes&#39;, &#39;listed&#39;, &#39;gem&#39;, &#39;.&#39;, &#39;imagine&#39;, &#39;movie&#39;, &#39;joe&#39;, &#39;piscopo&#39;, &#39;actually&#39;, &#39;funny&#39;, &#39;!&#39;, &#39;maureen&#39;, &#39;stapleton&#39;, &#39;scene&#39;, &#39;stealer&#39;, &#39;.&#39;, &#39;moroni&#39;, &#39;character&#39;, &#39;absolute&#39;, &#39;scream&#39;, &#39;.&#39;, &#39;watch&#39;, &#39;alan&#39;, &#39;``&#39;, &#39;skipper&#39;, &#34;&#39;&#39;&#34;, &#39;hale&#39;, &#39;jr.&#39;, &#39;police&#39;, &#39;sgt&#39;, &#39;.&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Punctuation">Punctuation<a class="anchor-link" href="#Punctuation"> </a></h3><p>We also want to get of all tokens that are not composed of letters (e.g. punctuation and numbers). We can check if a words is only composed of alphabetic letters with the <code>isalpha()</code> and filter with it:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;movie&#39;, &#39;gets&#39;, &#39;respect&#39;, &#39;sure&#39;, &#39;lot&#39;, &#39;memorable&#39;, &#39;quotes&#39;, &#39;listed&#39;, &#39;gem&#39;, &#39;imagine&#39;, &#39;movie&#39;, &#39;joe&#39;, &#39;piscopo&#39;, &#39;actually&#39;, &#39;funny&#39;, &#39;maureen&#39;, &#39;stapleton&#39;, &#39;scene&#39;, &#39;stealer&#39;, &#39;moroni&#39;, &#39;character&#39;, &#39;absolute&#39;, &#39;scream&#39;, &#39;watch&#39;, &#39;alan&#39;, &#39;skipper&#39;, &#39;hale&#39;, &#39;police&#39;, &#39;sgt&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Stemming">Stemming<a class="anchor-link" href="#Stemming"> </a></h3><p>As a final step we want to trim the words to the stem. This helps drastically decrease the vocabulary size and maps similar/same words onto the same word. E.g. plural/singular words or different forms of verbs:</p>
<ul>
<li>pen, pens --&gt; pen</li>
<li>happy, happier --&gt; happi</li>
<li>go, goes --&gt; go</li>
</ul>
<p>There are several languages available in nltk since this is a <strong>language dependant process</strong>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">SnowballStemmer</span><span class="o">.</span><span class="n">languages</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>(&#39;arabic&#39;, &#39;danish&#39;, &#39;dutch&#39;, &#39;english&#39;, &#39;finnish&#39;, &#39;french&#39;, &#39;german&#39;, &#39;hungarian&#39;, &#39;italian&#39;, &#39;norwegian&#39;, &#39;porter&#39;, &#39;portuguese&#39;, &#39;romanian&#39;, &#39;russian&#39;, &#39;spanish&#39;, &#39;swedish&#39;)
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Applied to the text sample this yields:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">stemmer</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;movi&#39;, &#39;get&#39;, &#39;respect&#39;, &#39;sure&#39;, &#39;lot&#39;, &#39;memor&#39;, &#39;quot&#39;, &#39;list&#39;, &#39;gem&#39;, &#39;imagin&#39;, &#39;movi&#39;, &#39;joe&#39;, &#39;piscopo&#39;, &#39;actual&#39;, &#39;funni&#39;, &#39;maureen&#39;, &#39;stapleton&#39;, &#39;scene&#39;, &#39;stealer&#39;, &#39;moroni&#39;, &#39;charact&#39;, &#39;absolut&#39;, &#39;scream&#39;, &#39;watch&#39;, &#39;alan&#39;, &#39;skipper&#39;, &#39;hale&#39;, &#39;polic&#39;, &#39;sgt&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Preprocessing-function">Preprocessing function<a class="anchor-link" href="#Preprocessing-function"> </a></h3>
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">preprocessing</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">language</span><span class="o">=</span><span class="s1">&#39;english&#39;</span><span class="p">,</span> <span class="n">stemming</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    preprocess a string and return processed tokens.</span>
<span class="sd">    args:</span>
<span class="sd">        text: text string</span>
<span class="sd">    return:</span>
<span class="sd">        tokens: list of processed and cleaned words</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">stop_words</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">stopwords</span><span class="o">.</span><span class="n">words</span><span class="p">(</span><span class="n">language</span><span class="p">))</span>
    <span class="n">stemmer</span> <span class="o">=</span> <span class="n">SnowballStemmer</span><span class="p">(</span><span class="n">language</span><span class="p">)</span>    
    
    <span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">word_tokenize</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">stop_words</span><span class="p">]</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tokens</span> <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">isalpha</span><span class="p">()]</span>
    <span class="k">if</span> <span class="n">stemming</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">stemmer</span><span class="o">.</span><span class="n">stem</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">tokens</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we can apply these steps to all texts. We use the <code>apply</code> function of pandas which applies a function to every entry in a DataFrame column. Since we registered <code>tqdm</code> we can use the <code>progress_apply</code> function which uses <code>apply</code> and adds a progress bar to it.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;text_processed_stemmed&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">progress_apply</span><span class="p">(</span><span class="n">preprocessing</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>progress: 100%|██████████| 50000/50000 [03:17&lt;00:00, 252.92it/s]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Part-3:-Vector-encoding">Part 3: Vector encoding<a class="anchor-link" href="#Part-3:-Vector-encoding"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Vectorizer">Vectorizer<a class="anchor-link" href="#Vectorizer"> </a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we cleaned up and tokenized the text corpus we are now ready to encode the texts in vectors. In class we had a look at simple <strong>one-hot encodings</strong> that can be extended to count encodings and <strong>TF-IDF encodings</strong>.</p>
<p>Scikit-learn comes with functions to do both count and TF-IDF encodings on text. The interface is very similar to the classifier just the <code>predict</code> step is replace with <code>transform</code>:</p>
<div class="highlight"><pre><span></span><span class="n">count_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">your_settings</span><span class="p">)</span>
<span class="n">count_vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">your_dataset</span><span class="p">)</span>
<span class="n">vec</span> <span class="o">=</span> <span class="n">count_vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="s1">&#39;your_text&#39;</span><span class="p">)</span>
</pre></div>
<p>This creates a vectorizer that can transform texts to vectors. We can also limit the number of words take into account when building the vector. This limits the vector size and cuts off words that occur rarely. If you set <code>max_features=10000</code> only the 10000 most occurring words are used to build the vector and all rare words are excluded. This means that the encoding vector then has a dimension of 10000. For now we take all words (<code>max_features=None</code>). Since we used our own tokenizer and preprocessing step we overwrite the standard steps in the vectorizer library with the <code>vec_default_settings</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">vec_default_settings</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;analyzer&#39;</span><span class="p">:</span><span class="s1">&#39;word&#39;</span><span class="p">,</span> <span class="s1">&#39;tokenizer&#39;</span><span class="p">:</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;preprocessor&#39;</span><span class="p">:</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="s1">&#39;token_pattern&#39;</span><span class="p">:</span><span class="kc">None</span><span class="p">,}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tfidf_vec</span> <span class="o">=</span> <span class="n">TfidfVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">vec_default_settings</span><span class="p">)</span>
<span class="n">count_vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">vec_default_settings</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's test both vectorizers on a small, dummy dataset with <strong>4 documents</strong>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">corpus</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">[</span><span class="s1">&#39;this&#39;</span><span class="p">,</span><span class="s1">&#39;is&#39;</span><span class="p">,</span><span class="s1">&#39;the&#39;</span><span class="p">,</span><span class="s1">&#39;first&#39;</span><span class="p">,</span><span class="s1">&#39;document&#39;</span><span class="p">,</span><span class="s1">&#39;in&#39;</span><span class="p">,</span><span class="s1">&#39;the&#39;</span><span class="p">,</span><span class="s1">&#39;corpus&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;this&#39;</span><span class="p">,</span><span class="s1">&#39;document&#39;</span><span class="p">,</span><span class="s1">&#39;is&#39;</span><span class="p">,</span><span class="s1">&#39;the&#39;</span><span class="p">,</span><span class="s1">&#39;second&#39;</span><span class="p">,</span><span class="s1">&#39;document&#39;</span><span class="p">,</span><span class="s1">&#39;in&#39;</span><span class="p">,</span><span class="s1">&#39;the&#39;</span><span class="p">,</span><span class="s1">&#39;corpus&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;and&#39;</span><span class="p">,</span><span class="s1">&#39;this&#39;</span><span class="p">,</span><span class="s1">&#39;is&#39;</span><span class="p">,</span><span class="s1">&#39;the&#39;</span><span class="p">,</span><span class="s1">&#39;third&#39;</span><span class="p">,</span><span class="s1">&#39;one&#39;</span><span class="p">,</span><span class="s1">&#39;in&#39;</span><span class="p">,</span><span class="s1">&#39;this&#39;</span><span class="p">,</span><span class="s1">&#39;corpus&#39;</span><span class="p">],</span>
    <span class="p">[</span><span class="s1">&#39;is&#39;</span><span class="p">,</span><span class="s1">&#39;this&#39;</span><span class="p">,</span><span class="s1">&#39;the&#39;</span><span class="p">,</span><span class="s1">&#39;first&#39;</span><span class="p">,</span><span class="s1">&#39;document&#39;</span><span class="p">,</span><span class="s1">&#39;in&#39;</span><span class="p">,</span><span class="s1">&#39;this&#39;</span><span class="p">,</span><span class="s1">&#39;corpus&#39;</span><span class="p">],</span>
<span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we fit a count vectorizer to the data.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">count_vec</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>CountVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;,
                dtype=&lt;class &#39;numpy.int64&#39;&gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;,
                lowercase=True, max_df=1.0, max_features=None, min_df=1,
                ngram_range=(1, 1),
                preprocessor=&lt;function &lt;lambda&gt; at 0x123e15bf8&gt;,
                stop_words=None, strip_accents=None, token_pattern=None,
                tokenizer=&lt;function &lt;lambda&gt; at 0x123e15ae8&gt;, vocabulary=None)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Once the a vectorizer is fitted, we can investigate the vocabulary. It is a dictionary that points each word to the index in the vector it corresponds to. For example the word <code>'this'</code> corresponds to the 10+1-nth (+1 because we start counting at zero) entry in the vector and the word <code>'and'</code> corresponds to the the first entry.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">count_vec</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>11</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">count_vec</span><span class="o">.</span><span class="n">vocabulary_</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;this&#39;: 10,
 &#39;is&#39;: 5,
 &#39;the&#39;: 8,
 &#39;first&#39;: 3,
 &#39;document&#39;: 2,
 &#39;in&#39;: 4,
 &#39;corpus&#39;: 1,
 &#39;second&#39;: 7,
 &#39;and&#39;: 0,
 &#39;third&#39;: 9,
 &#39;one&#39;: 6}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now we can transform the corpus and get a list of vectors in the form of a matrix (each row corresponds to a document vector):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">count_vec</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[[0 1 1 1 1 1 0 0 2 0 1]
 [0 1 2 0 1 1 0 1 2 0 1]
 [1 1 0 0 1 1 1 0 1 1 2]
 [0 1 1 1 1 1 0 0 1 0 2]]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If we now do the same thing with the TF-IDF vectorizer we see that the output looks different:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tfidf_vec</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">tfidf_vec</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[[0.         0.29137467 0.35639305 0.44021632 0.29137467 0.29137467
  0.         0.         0.58274934 0.         0.29137467]
 [0.         0.23798402 0.58217725 0.         0.23798402 0.23798402
  0.         0.45604658 0.47596805 0.         0.23798402]
 [0.43943636 0.22931612 0.         0.         0.22931612 0.22931612
  0.43943636 0.         0.22931612 0.43943636 0.45863224]
 [0.         0.29137467 0.35639305 0.44021632 0.29137467 0.29137467
  0.         0.         0.29137467 0.         0.58274934]]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>The shape of the matrix is the same.</li>
<li>Instead of integers (corresponding to counts) we have continous values.</li>
<li>Elements that occur in multilple documents have lower scores than those appearing in fewer.</li>
</ul>
<p>This should just illustrate how count and TF-IDF vectorizer work. Now let's apply this to our dataset and create encodigs with <code>100000</code> words:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="n-grams">n-grams<a class="anchor-link" href="#n-grams"> </a></h3><p>When we use a count or TF-IDF vectorizer we through away all sequential information in the texts. From the vector encodings above we could not reconstruct the original sentences. For this reason these encodings are called Bag-of-Words encodings (all words go in a bag and are shuffeled). However, sequential information can be important for the meaning of a sentence. As an example imagine the sentence:</p>
<div class="highlight"><pre><span></span><span class="n">text</span> <span class="o">=</span> <span class="s1">&#39;The movie was good and not bad.&#39;</span>
</pre></div>
<p>It is important to know if the word <code>'not'</code> is in front of <code>'good'</code> or <code>'bad'</code> for determining the sentiment of the sentence. We can preserve some of that information by using n-grams. Instead of just encoding single words we can also encode tuple, triplets etc. called n-grams. The n encodes how many words we bundle together.</p>
<p>The vectorizers can do this for us if we provide them a range of n's we want to include. In the following example we encode the text in 1- and 2-grams.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">count_vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span> <span class="o">**</span><span class="n">vec_default_settings</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">count_vec</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>CountVectorizer(analyzer=&#39;word&#39;, binary=False, decode_error=&#39;strict&#39;,
                dtype=&lt;class &#39;numpy.int64&#39;&gt;, encoding=&#39;utf-8&#39;, input=&#39;content&#39;,
                lowercase=True, max_df=1.0, max_features=None, min_df=1,
                ngram_range=(1, 2),
                preprocessor=&lt;function &lt;lambda&gt; at 0x123e15bf8&gt;,
                stop_words=None, strip_accents=None, token_pattern=None,
                tokenizer=&lt;function &lt;lambda&gt; at 0x123e15ae8&gt;, vocabulary=None)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that this drastically increases the vocabulary.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">count_vec</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>30</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now the vocabulary also contains word tuples next to the words:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">count_vec</span><span class="o">.</span><span class="n">vocabulary_</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>{&#39;this&#39;: 25,
 &#39;is&#39;: 11,
 &#39;the&#39;: 18,
 &#39;first&#39;: 6,
 &#39;document&#39;: 3,
 &#39;in&#39;: 8,
 &#39;corpus&#39;: 2,
 &#39;this is&#39;: 28,
 &#39;is the&#39;: 12,
 &#39;the first&#39;: 20,
 &#39;first document&#39;: 7,
 &#39;document in&#39;: 4,
 &#39;in the&#39;: 9,
 &#39;the corpus&#39;: 19,
 &#39;second&#39;: 16,
 &#39;this document&#39;: 27,
 &#39;document is&#39;: 5,
 &#39;the second&#39;: 21,
 &#39;second document&#39;: 17,
 &#39;and&#39;: 0,
 &#39;third&#39;: 23,
 &#39;one&#39;: 14,
 &#39;and this&#39;: 1,
 &#39;the third&#39;: 22,
 &#39;third one&#39;: 24,
 &#39;one in&#39;: 15,
 &#39;in this&#39;: 10,
 &#39;this corpus&#39;: 26,
 &#39;is this&#39;: 13,
 &#39;this the&#39;: 29}</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The encodings look very similar but are larger due to the larger vocabulary:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">count_vec</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">toarray</span><span class="p">())</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[[0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 0 0 0 2 1 1 0 0 0 0 1 0 0 1 0]
 [0 0 1 2 1 1 0 0 1 1 0 1 1 0 0 0 1 1 2 1 0 1 0 0 0 1 0 1 0 0]
 [1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0 0 1 1 1 2 1 0 1 0]
 [0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 0 0 0 1 0 1 0 0 0 0 2 1 0 0 1]]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Vectorize-dataset">Vectorize dataset<a class="anchor-link" href="#Vectorize-dataset"> </a></h3><p>Now we want to encode the real text. We put an upper limit on the vocabulary size. Since there a lot of unique words in our corpus there are a lot of combinations of words.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">max_features</span><span class="o">=</span><span class="mi">100000</span>
<span class="n">ngrams</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>

<span class="n">count_vec</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="n">ngrams</span><span class="p">,</span> <span class="o">**</span><span class="n">vec_default_settings</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the example above we used the <code>fit</code> and <code>transform</code> function. We can avoid these two steps with the combined function <code>fit_transform</code>. First we need to split the dataset:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">text_train</span> <span class="o">=</span> <span class="n">df_imdb</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;train_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;text_processed_stemmed&#39;</span><span class="p">]</span>
<span class="n">text_test</span> <span class="o">=</span> <span class="n">df_imdb</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;train_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="s1">&#39;text_processed_stemmed&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">count_vec</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">text_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">count_vec</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">text_test</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This yields a vocabulary with <code>100000</code> entries:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">count_vec</span><span class="o">.</span><span class="n">vocabulary_</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>100000</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Looking at the shape of the returned matrix we see that it still has as many rows as the input but now has <code>100000</code> entries per row (the feature vector).</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(25000, 100000)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Part-4:-Na&#239;ve-Bayes-classifier">Part 4: Na&#239;ve Bayes classifier<a class="anchor-link" href="#Part-4:-Na&#239;ve-Bayes-classifier"> </a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we featurised the text we can train a model on it. In this section we will use a Naïve Bayes classifier to determine whether a review is positive or negative. Firs we split the labels of the training and test set.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_train</span> <span class="o">=</span> <span class="n">df_imdb</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;train_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;sentiment&#39;</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">df_imdb</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_imdb</span><span class="p">[</span><span class="s1">&#39;train_label&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="s1">&#39;sentiment&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can initialise a Naïve Bayes classifier the same way we initialised the Random Forest models. Also the fit/predict interface is the same.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">nb_clf</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>
<span class="n">nb_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CPU times: user 90.6 ms, sys: 21.5 ms, total: 112 ms
Wall time: 130 ms
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">nb_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can calculate the prediction accuracy on the test set:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.85192</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Random-Forest">Random Forest<a class="anchor-link" href="#Random-Forest"> </a></h3><p>We can compare the Naïve Bayes model with a Random Forest on the same task with the same input data.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rf_clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>
<span class="n">rf_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CPU times: user 1min 23s, sys: 1.29 s, total: 1min 24s
Wall time: 27.2 s
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion=&#39;gini&#39;, max_depth=None, max_features=&#39;auto&#39;,
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=-1, oob_score=False, random_state=None, verbose=0,
                       warm_start=False)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">rf_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.85328</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can see that we get similar performance while being <strong>~1000x faster</strong>!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Custom-prediction">Custom prediction<a class="anchor-link" href="#Custom-prediction"> </a></h3><p>Now that we have a model we want to use it to make some custom predictions. We can build an easy pipeline that preprocesses, vectorises and predicts:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;This movie sucked!!&#39;</span><span class="p">,</span>
         <span class="s1">&#39;This movie is awesome :)&#39;</span><span class="p">,</span>
         <span class="s1">&#39;I did not like that movie at all.&#39;</span><span class="p">,</span>
         <span class="s1">&#39;The movie was boring.&#39;</span><span class="p">]</span>

<span class="n">enc</span> <span class="o">=</span> <span class="n">count_vec</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">preprocessing</span><span class="p">(</span><span class="n">text</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">nb_clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">enc</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[&#39;neg&#39; &#39;pos&#39; &#39;neg&#39; &#39;neg&#39;]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Exercise-3">Exercise 3<a class="anchor-link" href="#Exercise-3"> </a></h3><p>Retrain the model with a TF-IDF encoding instead of the count encoding. Experiment with the n-gram setting and run the experiment with the following settings: <code>(1,1)</code>, <code>(1,2)</code>, <code>(1,3)</code> and <code>(1,4)</code>.</p>

</div>
</div>
</div>
</div>
 

