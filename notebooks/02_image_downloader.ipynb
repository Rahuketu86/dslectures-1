{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp image_downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image downloader\n",
    "\n",
    "> Useful helper function to download images from Google search. Adapted from the fastai repository ([link](https://github.com/fastai/fastai/blob/eb6b2eab34cc5a65e338df1cec91fb7296981048/fastai/widgets/image_downloader.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "try:\n",
    "    from fastai.core import *\n",
    "    from fastai.vision.data import *\n",
    "except:\n",
    "    print(\"fastai not installed. To use the ImageDownloader widget install fastai first.\")\n",
    "from urllib.parse import quote\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "\n",
    "class ImageDownloader:\n",
    "    def __init__(self, data_path, dataset_name):\n",
    "        \"\"\"The ImageDownloader helps download images from the google image search page\"\"\"\n",
    "        self._path = Path(data_path)\n",
    "        self._dataset_path = self.path/dataset_name\n",
    "        self._dataset_name = dataset_name\n",
    "        \n",
    "        os.makedirs(self._path, exist_ok=True)\n",
    "        os.makedirs(self._dataset_path, exist_ok=True)\n",
    "        \n",
    "    def add_images_to_class(self, class_name, google_query, n_images=1000):\n",
    "        \"\"\"Add new images to the image class with a Google search query.\"\"\"\n",
    "        class_path = self._dataset_path/class_name\n",
    "        url = _search_url(google_query)\n",
    "        html = self.get_google_image_html(url)\n",
    "        img_urls = self.get_img_urls_from_html(html)\n",
    "        print(f'{len(img_urls)} image links found on Google image search for the query \"{google_query}\".')\n",
    "        img_fnames = _download_images(class_path, img_urls)\n",
    "        print(f'{len(img_fnames)} images now available in class {class_name}.')\n",
    "        \n",
    "        \n",
    "    def get_google_image_html(self, url):\n",
    "        \"\"\"Get the html code of the Google Image Search.\"\"\"\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--headless\")\n",
    "        options.add_argument('--no-sandbox')\n",
    "        try: \n",
    "            driver = webdriver.Chrome(chrome_options=options)\n",
    "        except: \n",
    "            print(\"\"\"Error initializing chromedriver. \n",
    "                  Check if it's in your path by running `which chromedriver`\"\"\")\n",
    "        driver.set_window_size(1440, 900)\n",
    "        driver.get(url)\n",
    "        old_height = 0\n",
    "        for i in range(n_images // 100 + 1):\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(1.0 + random.random())\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == old_height:\n",
    "                try:\n",
    "                    button = driver.find_elements_by_xpath(\"//input[@type='button' and @value='Show more results']\")[0]\n",
    "                    button.click()\n",
    "                except:\n",
    "                    pass    \n",
    "            old_height = new_height\n",
    "        return driver.page_source\n",
    "    \n",
    "    def get_img_urls_from_html(self, html):\n",
    "        \n",
    "        bs = BeautifulSoup(html, 'html.parser')\n",
    "        img_tags = bs.find_all('img')\n",
    "        urls = []\n",
    "        \n",
    "        for tag in img_tags:\n",
    "            if tag.has_attr('data-src'):\n",
    "                urls.append(tag['data-src'])\n",
    "        return urls\n",
    "\n",
    "def _download_images(label_path:PathOrStr, img_urls:list, max_workers:int=defaults.cpus, timeout:int=4) -> FilePathList:\n",
    "    \"\"\"\n",
    "    Downloads images in `img_tuples` to `label_path`. \n",
    "    If the directory doesn't exist, it'll be created automatically.\n",
    "    Uses `parallel` to speed things up in `max_workers` when the system has enough CPU cores.\n",
    "    If something doesn't work, try setting up `max_workers=0` to debug.\n",
    "    \"\"\"\n",
    "    os.makedirs(Path(label_path), exist_ok=True)\n",
    "    parallel( partial(_download_single_image, label_path, timeout=timeout), img_urls, max_workers=max_workers)\n",
    "    return get_image_files(label_path)\n",
    "\n",
    "def _download_single_image(label_path:Path, img_url:tuple, i:int, timeout:int=4) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a single image from Google Search results to `label_path`\n",
    "    given an `img_tuple` that contains `(fname, url)` of an image to download.\n",
    "    `i` is just an iteration number `int`. \n",
    "    \"\"\"\n",
    "    fname = img_url.split('%')[1].split('&')[0]+'.png'\n",
    "    download_url(img_url, label_path/fname, timeout=timeout)\n",
    "    \n",
    "def _search_url(search_term:str, size:str='>400*300', format:str='jpg') -> str:\n",
    "    \"Return a Google Images Search URL for a given search term.\"\n",
    "    return ('https://www.google.com/search?q=' + quote(search_term) +\n",
    "            '&espv=2&biw=1366&bih=667&site=webhp&source=lnms&tbm=isch' +\n",
    "            _url_params(size, format) + '&sa=X&ei=XosDVaCXD8TasATItgE&ved=0CAcQ_AUoAg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
